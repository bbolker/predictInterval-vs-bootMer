---
title: soib summary
---

```{r setup, message=FALSE}
options(bitmapType = "cairo")
library(tidyverse)
library(ggplot2); theme_set(theme_bw())
## https://stackoverflow.com/questions/53750310/how-to-change-default-color-scheme-in-ggplot2
scale_colour_discrete <- function(...) {
  scale_colour_viridis_d(...)
}
scale_fill_discrete <- function(...) {
  scale_fill_viridis_d(...)
}
```

The goal here is to understand why `bootMer` and `merTools::predictInterval` are giving substantially different answers for the standard errors of predictions. Briefly (see below), `predictInterval` samples from the (sampling) covariance matrix of the fixed effects ($\Sigma(\hat \beta)$) and the ([approximate] conditional covariance matrices of the random effects ($\Sigma(\tilde b)$). Possible differences between `bootMer` and `predictInterval` include:

* bad estimation of the $\beta$ covariance matrix (see e.g. GitHub issues about the `use.hessian` argument of `vcov()` ...). **Test**: compare `vcov(.,use.hessian=TRUE)`, `vcov(.,use.hessian=FALSE)`, and the covariance matrix of $\beta$ values returned from `bootMer`
* departure from multivariate normality of $\Sigma(\hat \beta)$? (see above)
* conditioning on $\hat \theta$ (estimated random effect covariance parameters): use `bootMer` output to investigate the correlations between $\hat \theta$ and the uncertainties of $\{\tilde b, \hat \beta\}$? (This conditioning will only matter if the uncertainty in $\hat \theta$ is large *and* correlated with the things that directly determine uncertainty in the predictions ...)
* considering `use.u = TRUE` and `use.u = FALSE` (i.e. when bootstrapping, do we condition on $\tilde b$ or simulate from the unconditional distribution based on $\hat \theta$?) (I think that `use.u = FALSE` in `bootMer` corresponds to `re.form = NA` in `predictInterval` ...)
* are there issues surrounding `re.form = NA`, i.e. population-level prediction?
* does `predictInterval` really give *confidence intervals* and not *prediction intervals*? (I think so: the `include.resid.var` argument, which is TRUE by default, suggests that we can get prediction intervals for LMMs if we want, but we have a GLMM and there doesn't seem to be a corresponding argument for GLMMs - in any case it would be weird with a binary response ...)
* in the example we're using `which = "fixed"`, which should (?????) be equivalent to *ignoring* RE variation ...

Some things to try:

* compare `bootMer` vs `predictInterval` for a variety of cases - how good is it in the best-case scenario, when does it go bad? (i.e. LMM; binomial GLMM; binomial GLMM with cloglog link ...) With/without `use.u`, `re.form = NA`, etc. ...
* compare with values based on `predict.merMod(., se.fit = TRUE)` (need to fix bug associated with using `newdata` here, although can work around it by predicting everything for small data sets)
* compare with values from `glmmTMB` with `predict(., se.fit = TRUE)`; this does use a Wald approximation, but doesn't condition on $\hat \theta$ ...
* compare with the simpler `sqrt(diag(X %*% V %*% t(X)))` computation, which should (?) be equivalent to `which = "fixed"` ... ? (This is easy enough to do by hand with `X <- model.matrix(.)`, `V <- vcov(.)`, but may also be encoded in some package somewhere.  Doing it by hand might be more transparent?)

Here are the results from the original example: this samples 100,000 values from the full data set, fits the model (a binomial GLMM with a cloglog link) and compares the prediction intervals for 60 cases (every month and time group) between `predictInterval` and `bootMer` with 100 sims (I bumped this up a little from the original).

```{r load_dat}
L <- load("soib_reprex_orig.rda")  ## original example
L2 <- load("predint_ci.rda")       ## results of various experiments
load("model_lme4.rda")             ## model_lme4, data_to_pred
source("funs.R")
```

Comparing with 'full' (`n=1e5`) data set:

```{r compare-lme4f, message = FALSE}
plot_intervals(ci_lme4_f)
```

`XVXt` (direct computation of confidence intervals via $\sqrt{\textrm{Diag}(\boldsymbol{X} \boldsymbol{V} \boldsymbol{X}^\top)}$) overlaps completely with `predictInterval.fixed`, as we can see by spreading things out with `facet_wrap`:

```{r compare-lme4f-facet, message = FALSE, fig.width = 10}
plot_intervals(ci_lme4_f) + facet_wrap(~ method)
```

Why is the bias there? (Previous issues were about the use of `include.resid.var`, which we'll skip over for now because (1) we actually we confidence rather than prediction intervals and (2) it seems as though (???) `predictInterval` does some bogus things with prediction intervals for GLMMs ...

* intervals from original model (based on 1e4 samples, 100 bootstraps):

```{r compare-lme4, message = FALSE, fig.width = 10}
plot_intervals(ci_lme4)
```

Hmmm. The bias is similar, but harder to see 

```{r check-levs}
levs <- c(0.48, 0.8)
names(levs) <- levs ## for map_dfr naming
predfun <- function(level) {
    merTools::predictInterval(model_lme4,
                    newdata = data_to_pred,
                    type = "linear.prediction",
                    which = "fixed",
                    level = level)
}

lev_cmp <- suppressWarnings(
    purrr::map_dfr(levs, predfun,
                   .id = "level")) |>
    arrange(level, fit) |>
    mutate(rank = seq(n()), .by = level)

ggplot(lev_cmp) +
    geom_ribbon(aes(x = rank, ymin = lwr, ymax = upr,  fill = level),
                alpha = 0.3, colour = NA)
```


```{r check-incresvar}
incres <- c(TRUE, FALSE)
names(incres) <- as.character(incres) ## for map_dfr naming
predfun2 <- function(ir) {
    merTools::predictInterval(model_lme4,
                    newdata = data_to_pred,
                    type = "linear.prediction",
                    which = "fixed",
                    level = 0.8,
                    include.resid.var = ir)
}

ir_cmp <- suppressWarnings(
    purrr::map_dfr(incres, predfun2,
                   .id = "incresvar")) |>
    arrange(incresvar, fit) |>
    mutate(rank = seq(n()), .by = incresvar)

ggplot(ir_cmp) +
    geom_ribbon(aes(x = rank, ymin = lwr, ymax = upr,  fill = incresvar),
                alpha = 0.3, colour = NA)
```

Is this it????  Do I have to go back and re-run everything with `include.resid.var = TRUE` ?

* intervals from a complex binomial fit (`Contraception` data set from `mlmRev` package, `use ~ livch*poly(age, 2) + (urban|district)`, logit link):

```{r compare-cc}
plot_intervals(ci_cc)
```

* intervals from the standard `sleepstudy` example (LMM):

```{r compare-ss}
plot_intervals(ci_ss)
```

* intervals from a simulated cloglog-binomial example, with the same structure (sample size etc.) as the previous example:

```{r compare-cc2}
plot_intervals(ci_cc2)
```

* intervals from a simulated example, as above but much larger (i.e. less uncertainty in RE parameters, so differences should be smaller (50 groups, 20 samples per group, a single Normally distributed covariate):

```{r compare-cc3}
plot_intervals(ci_cc3)
```


From `?merTools::predictInterval`:

> To generate a prediction interval, the function first computes a simulated distribution of all of the parameters in the model. For the random, or grouping, effects, this is done by sampling from a multivariate normal distribution which is defined by the BLUP estimate provided by ‘ranef’ and the associated variance-covariance matrix for each observed level of each grouping terms. For each grouping term, an array is build that has as many rows as there are levels of the grouping factor, as many columns as there are predictors at that level (e.g. an intercept and slope), and is stacked as high as there are number of simulations. These arrays are then multiplied by the new data provided to the function to produce a matrix of yhat values. The result is a matrix of the simulated values of the linear predictor for each observation for each simulation. Each grouping term has such a matrix for each observation. These values can be added to get the estimate of the fitted value for the random effect terms, and this can then be added to a matrix of simulated values for the fixed effect level to come up with ‘n.sims’ number of possible yhat values for each observation. 
